{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "licao1_convkeras-ead",
      "provenance": [],
      "authorship_tag": "ABX9TyPwdfyGiAg9RgGKYOu4I9JM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaelrlima/aula8_rede_neural_convolucional/blob/main/licao1_convkeras_ead.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhqDEAPkdxK8"
      },
      "source": [
        "# PSI5796 Lição de casa "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH-UCLJfNGmG"
      },
      "source": [
        "O programa 1 utiliza 60000 imagens de treinamento. Modifique-o para que utilize apenas 6000 primeiras imagens de treino, isto é 10% dos dados de treino\n",
        "iniciais. Não tem problema se alguns dígitos possuem um pouco mais exemplos de treinamento do\n",
        "que outros. Ajuste o programa para obter a menor taxa de erro possível quando o programa classifica as 10000 imagens de teste. Qual foi a taxa de erro obtida? O seu vídeo deve mostrar claramente\n",
        "que está utilizando apenas 6000 imagens de treino, imprimindo AX.shape[0] e AY.shape[0]. Também deve deixar claro no vídeo a taxa de erro obtida. Depois, treine com 600 imagens de treino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLQQLnGtLg4K"
      },
      "source": [
        "#cnn1.py - pos2021\n",
        "import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
        "# Se acontecer erro \"Failed to get convolution algorithm” coloque o comando abaixo\n",
        "# https://github.com/tensorflow/tensorflow/issues/43174\n",
        "# https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in\n",
        "#os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "import tensorflow.keras as keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from keras import optimizers\n",
        "import numpy as np; import sys; import os; from time import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVw-oON5NsyJ"
      },
      "source": [
        "(AX, AY), (QX, QY) = mnist.load_data() # AX [60000,28,28] AY [60000,]\n",
        "AX=255-AX; QX=255-QX"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylOQNsjJg9xX"
      },
      "source": [
        "#### FORMA 1 DE OBTER AS 6000 PRIMEIROS REGISTROS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gQBDYRdPPGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddc9fdf-8860-4f9e-91c3-9a3800040468"
      },
      "source": [
        "AX1 = AX[:6000,:,:]\n",
        "AY1 = AY[:6000]\n",
        "\n",
        "print(\"-------- dataset original -------\")\n",
        "print(\"AX = \",AX.shape)\n",
        "print(\"AY = \",AY.shape)\n",
        "print(\"-------- dataset dividido -------\")\n",
        "print(\"AX1 = \",AX1.shape)\n",
        "print(\"AY1 = \",AY1.shape)\n",
        "\n",
        "AX = AX1\n",
        "AY = AY1\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------- dataset original -------\n",
            "AX =  (60000, 28, 28)\n",
            "AY =  (60000,)\n",
            "-------- dataset dividido -------\n",
            "AX1 =  (6000, 28, 28)\n",
            "AY1 =  (6000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af6FEDiDhGNR"
      },
      "source": [
        "#### FORMA 2 DE OBTER AS 6000 PRIMEIROS REGISTROS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucYvZYz-Q0AT"
      },
      "source": [
        "#AX1 = np.array_split(AX,10)\n",
        "#AY1 = np.array_split(AY,10)\n",
        "\n",
        "#print(\"-------- dataset original -------\")\n",
        "#print(\"AX = \",AX.shape)\n",
        "#print(\"AY = \",AY.shape)\n",
        "#print(\"-------- dataset dividido -------\")\n",
        "#print(\"AX1 = \",AX1[0].shape)\n",
        "#print(\"AY1 = \",AY1[0].shape)\n",
        "\n",
        "#AX = AX1[0]\n",
        "#AY = AY1[0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kqiJ7L5inES"
      },
      "source": [
        "#### Normalizando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KgrFqkZRZv7"
      },
      "source": [
        "nclasses = 10\n",
        "AY2 = keras.utils.to_categorical(AY, nclasses) # 3 -> 0001000000\n",
        "QY2 = keras.utils.to_categorical(QY, nclasses)\n",
        "nl, nc = AX.shape[1], AX.shape[2] #28, 28\n",
        "AX = AX.astype('float32') / 255.0 # 0 a 1\n",
        "QX = QX.astype('float32') / 255.0 # 0 a 1\n",
        "AX = np.expand_dims(AX,axis=3) # AX [60000,28,28,1]\n",
        "QX = np.expand_dims(QX,axis=3)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRcjqRFVhggm"
      },
      "source": [
        "#### Modelo Professor Aula\n",
        "Test loss: 0.0716\n",
        "Test accuracy: 98.35 %\n",
        "Test error: 1.65 %"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTcWL9NYTVqw"
      },
      "source": [
        "#model = Sequential() # 28x28\n",
        "#model.add(Conv2D(20, kernel_size=(5,5), activation='relu', input_shape=(nl, nc, 1) )) #24x24x20\n",
        "#model.add(MaxPooling2D(pool_size=(2,2))) #12x12x20\n",
        "#model.add(Conv2D(40, kernel_size=(5,5), activation='relu')) #8x8x40\n",
        "#model.add(MaxPooling2D(pool_size=(2,2))) #4x4x40\n",
        "#model.add(Flatten()) #640\n",
        "#model.add(Dense(1000, activation='relu')) #1000\n",
        "#model.add(Dense(nclasses, activation='softmax')) #10"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePle6v52iPoN"
      },
      "source": [
        "#### Modelo Ajustado 1\n",
        "Test loss: 0.0716\n",
        "Test accuracy: 98.49 %\n",
        "Test error: 1.51 %"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5WTlVOXiT7p"
      },
      "source": [
        "#model = Sequential() # 28x28\n",
        "#model.add(Conv2D(450, kernel_size=(5,5), padding='same', activation='relu', input_shape=(nl, nc, 1) )) #24x24x20\n",
        "#model.add(MaxPooling2D(strides=2)) #12x12x20\n",
        "#model.add(Conv2D(900, kernel_size=(5,5), padding='valid', activation='relu')) #8x8x40\n",
        "#model.add(MaxPooling2D(strides=2)) #4x4x40\n",
        "#model.add(Flatten()) #640\n",
        "#model.add(Dense(2000, activation='relu')) #1000\n",
        "#model.add(Dense(nclasses, activation='softmax')) #10"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmZenklh3iJi"
      },
      "source": [
        "#### Modelo Ajustado 2\n",
        "Test loss: 0.1070\n",
        "Test accuracy: 98.01 %\n",
        "Test error: 1.99 %\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNQy-FbS03go"
      },
      "source": [
        "#model = Sequential() # 28x28\n",
        "#model.add(Conv2D(32, kernel_size=(5,5), activation='relu', input_shape=(nl, nc, 1) )) #24x24x20\n",
        "#model.add(Conv2D(32, kernel_size=(5,5), activation='relu')) #8x8x40\n",
        "#model.add(MaxPooling2D(strides=2)) #12x12x20\n",
        "#model.add(Conv2D(64, kernel_size=(5,5), activation='relu')) #8x8x40\n",
        "#model.add(Conv2D(64, kernel_size=(5,5), activation='relu')) #8x8x40\n",
        "#model.add(MaxPooling2D(strides=2)) #4x4x40\n",
        "#model.add(Flatten()) #640\n",
        "#model.add(Dense(256, activation='relu')) #1000\n",
        "#model.add(Dense(128, activation='relu')) #1000\n",
        "#model.add(Dense(84, activation='relu')) #1000\n",
        "#model.add(Dense(nclasses, activation='softmax')) #10"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvcvV5fv4Yg-"
      },
      "source": [
        "#### Modelo Ajustado 3\n",
        "Test loss: 0.0791\n",
        "Test accuracy: 98.51 %\n",
        "Test error: 1.49 %"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__HIvo9u11tp"
      },
      "source": [
        "model = Sequential() # 28x28\n",
        "model.add(Conv2D(64, kernel_size=(5,5), padding='same', activation='relu', input_shape=(nl, nc, 1) )) #24x24x20\n",
        "model.add(Conv2D(64, kernel_size=(5,5), padding='valid', activation='relu')) #8x8x40\n",
        "model.add(MaxPooling2D(strides=2)) #12x12x20\n",
        "model.add(Conv2D(128, kernel_size=(5,5), padding='valid', activation='relu')) #8x8x40\n",
        "model.add(Conv2D(128, kernel_size=(5,5), padding='valid', activation='relu')) #8x8x40\n",
        "model.add(MaxPooling2D(strides=2)) #4x4x40\n",
        "model.add(Flatten()) #640\n",
        "model.add(Dense(256, activation='relu')) #1000\n",
        "model.add(Dense(128, activation='relu')) #1000\n",
        "model.add(Dense(84, activation='relu')) #1000\n",
        "model.add(Dense(nclasses, activation='softmax')) #10"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grNpxEqSiYsh"
      },
      "source": [
        "#### Otimizando e exportando o modelo em png"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvIEmJdZTV1g",
        "outputId": "2e5c780c-ad1b-470b-97a4-f2a8814026d6"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='cnn1.png', show_shapes=True); model.summary()\n",
        "opt=optimizers.Adam()\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 64)        102464    \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 128)         204928    \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 128)         409728    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 84)                10836     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                850       \n",
            "=================================================================\n",
            "Total params: 894,694\n",
            "Trainable params: 894,694\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KN432qgUMVH"
      },
      "source": [
        "#### Treinando o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuN7B1iSTwsj",
        "outputId": "084d99ad-ccd2-42b7-af01-18e86abcf375"
      },
      "source": [
        "t0=time()\n",
        "model.fit(AX, AY2, batch_size=100, epochs=30, verbose=2)\n",
        "t1=time(); print(\"Tempo de treino: %.2f s\"%(t1-t0))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60/60 - 4s - loss: 1.1548 - accuracy: 0.6013\n",
            "Epoch 2/30\n",
            "60/60 - 1s - loss: 0.2112 - accuracy: 0.9353\n",
            "Epoch 3/30\n",
            "60/60 - 1s - loss: 0.1257 - accuracy: 0.9637\n",
            "Epoch 4/30\n",
            "60/60 - 1s - loss: 0.0984 - accuracy: 0.9723\n",
            "Epoch 5/30\n",
            "60/60 - 1s - loss: 0.0681 - accuracy: 0.9775\n",
            "Epoch 6/30\n",
            "60/60 - 1s - loss: 0.0526 - accuracy: 0.9840\n",
            "Epoch 7/30\n",
            "60/60 - 1s - loss: 0.0433 - accuracy: 0.9872\n",
            "Epoch 8/30\n",
            "60/60 - 1s - loss: 0.0207 - accuracy: 0.9942\n",
            "Epoch 9/30\n",
            "60/60 - 1s - loss: 0.0262 - accuracy: 0.9933\n",
            "Epoch 10/30\n",
            "60/60 - 1s - loss: 0.0154 - accuracy: 0.9952\n",
            "Epoch 11/30\n",
            "60/60 - 1s - loss: 0.0236 - accuracy: 0.9918\n",
            "Epoch 12/30\n",
            "60/60 - 1s - loss: 0.0197 - accuracy: 0.9943\n",
            "Epoch 13/30\n",
            "60/60 - 1s - loss: 0.0091 - accuracy: 0.9977\n",
            "Epoch 14/30\n",
            "60/60 - 1s - loss: 0.0130 - accuracy: 0.9968\n",
            "Epoch 15/30\n",
            "60/60 - 1s - loss: 0.0124 - accuracy: 0.9960\n",
            "Epoch 16/30\n",
            "60/60 - 1s - loss: 0.0187 - accuracy: 0.9937\n",
            "Epoch 17/30\n",
            "60/60 - 1s - loss: 0.0249 - accuracy: 0.9920\n",
            "Epoch 18/30\n",
            "60/60 - 1s - loss: 0.0045 - accuracy: 0.9992\n",
            "Epoch 19/30\n",
            "60/60 - 1s - loss: 0.0043 - accuracy: 0.9983\n",
            "Epoch 20/30\n",
            "60/60 - 1s - loss: 0.0095 - accuracy: 0.9973\n",
            "Epoch 21/30\n",
            "60/60 - 1s - loss: 0.0197 - accuracy: 0.9940\n",
            "Epoch 22/30\n",
            "60/60 - 1s - loss: 0.0071 - accuracy: 0.9978\n",
            "Epoch 23/30\n",
            "60/60 - 1s - loss: 0.0134 - accuracy: 0.9963\n",
            "Epoch 24/30\n",
            "60/60 - 1s - loss: 0.0177 - accuracy: 0.9937\n",
            "Epoch 25/30\n",
            "60/60 - 1s - loss: 0.0023 - accuracy: 0.9993\n",
            "Epoch 26/30\n",
            "60/60 - 1s - loss: 4.5487e-04 - accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "60/60 - 1s - loss: 7.5964e-05 - accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "60/60 - 1s - loss: 3.4266e-04 - accuracy: 0.9998\n",
            "Epoch 29/30\n",
            "60/60 - 1s - loss: 0.0018 - accuracy: 0.9998\n",
            "Epoch 30/30\n",
            "60/60 - 1s - loss: 1.8444e-04 - accuracy: 1.0000\n",
            "Tempo de treino: 21.19 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyTMtF4nUQK3"
      },
      "source": [
        "#### Avaliando o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbuniDDaT1_x",
        "outputId": "33d3d413-59dd-4236-ed91-021ae35ba5dc"
      },
      "source": [
        "score = model.evaluate(QX, QY2, verbose=False)\n",
        "print('Test loss: %.4f'%(score[0]))\n",
        "print('Test accuracy: %.2f %%'%(100*score[1]))\n",
        "print('Test error: %.2f %%'%(100*(1-score[1])))\n",
        "#model.save('cnn1.h5')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.0791\n",
            "Test accuracy: 98.51 %\n",
            "Test error: 1.49 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWA1uMYKZ-qJ"
      },
      "source": [
        "### Resultados\n",
        "\n",
        "#### Utilizando a Base inteira 60.000 (Modelo Programa 1)<br>\n",
        "Test loss: 0.0589\n",
        "Test accuracy: 99.14 %\n",
        "Test error: 0.86 %\n",
        "\n",
        "#### Utilizando 10% 6.000  (Modelo Programa 1)<br>\n",
        "Test loss: 0.0716\n",
        "Test accuracy: 98.35 %\n",
        "Test error: 1.65 %\n",
        "\n",
        "#### Modelo 3 6.000 Ajustado 30 épocas\n",
        "Test loss: 0.0791\n",
        "Test accuracy: 98.51 %\n",
        "Test error: 1.49 %\n",
        "\n",
        "#### Modelo 3 600 Treinos\n",
        "Test loss: 0.5574\n",
        "Test accuracy: 91.93 %\n",
        "Test error: 8.07 %\n",
        "\n",
        "\n"
      ]
    }
  ]
}